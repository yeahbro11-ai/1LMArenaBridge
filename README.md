# LM Arena Bridge

## Description

A bridge to interact with LM Arena. This project provides an OpenAI compatible API endpoint that interacts with models on LM Arena, including experimental support for stealth models.

## Getting Started

### Prerequisites

- Python 3.x

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/CloudWaddie/LMArenaBridge.git
   ```
2. Navigate to the project directory:
   ```bash
   cd LMArenaBridge
   ```
3. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

## Usage

### 1. Get your Authentication Token

To use the LM Arena Bridge, you need to get your authentication token from the LM Arena website.

1.  Open your web browser and go to the LM Arena website.
2.  Send a message in the chat to any model.
3.  After the model responds, open the developer tools in your browser (usually by pressing F12).
4.  Go to the "Application" or "Storage" tab (the name may vary depending on your browser).
5.  In the "Cookies" section, find the cookies for the LM Arena site.
6.  Look for a cookie named `arena-auth-prod-v1` and copy its value. This is your authentication token.

### 2. Configure the Application

1.  In the root of the project, create a file named `config.json`.
2.  Add the following content to the `config.json` file:

    ```json
    {
      "auth_token": "YOUR_AUTH_TOKEN"
    }
    ```

3.  Replace `"YOUR_AUTH_TOKEN"` with the `arena-auth-prod-v1` token you copied from your browser.

### 3. Run the Application

Once you have configured your authentication token, you can run the application:

```bash
python src/main.py
```

The application will start a server on `localhost:8000`.

## Integration with OpenWebUI

You can use this project as a backend for [OpenWebUI](https://openwebui.com/), a user-friendly web interface for Large Language Models.

### Instructions

1.  **Run the LM Arena Bridge:**
    Make sure the `lmarenabridge` application is running.
    ```bash
    python src/main.py
    ```

2.  **Open OpenWebUI:**
    Open the OpenWebUI interface in your web browser.

3.  **Configure the OpenAI Connection:**
    - Go to your **Profile**.
    - Open the **Admin Panel**.
    - Go to **Settings**.
    - Go to **Connections**.
    - Modify the **OpenAI connection**.

4.  **Set the API Base URL:**
    - In the OpenAI connection settings, set the **API Base URL** to the URL of the LM Arena Bridge API, which is `http://localhost:8000/api/v1`.
    - You can leave the **API Key** field empty or enter any value. It is not used for authentication by the bridge itself.

5.  **Start Chatting:**
    You should now be able to select and chat with the models available on LM Arena through OpenWebUI.

### Context Awareness & Token Tracking

LMArenaBridge now surfaces context window details to help you avoid exhausting the conversation buffer:

- `GET /api/v1/models` returns each model's estimated context window along with a `context_status` block (usage, remaining capacity, and actionable guidance). Pass an optional `conversation_id` query parameter to see the active session's usage when selecting models.
- `POST /api/v1/chat/completions` responses and streaming chunks include `usage` and `context_status` objects so clients can display real-time token consumption (for both synchronous and streaming chats).
- `GET /api/v1/conversations/{conversation_id}/status` exposes the latest totals for an active conversation, making it easy to poll and present live context status in your UI.

Use these fields to warn users before they hit limits, switch to higher-capacity models, or reset chats proactively.

## Image Support

LMArenaBridge supports sending images to vision-capable models on LMArena. When you send a message with images to a model that supports image input, the images are automatically uploaded to LMArena's R2 storage and included in the request.

## Production Deployment

### Error Handling

LMArenaBridge includes comprehensive error handling for production use:

- **Request Validation**: Validates JSON format, required fields, and data types
- **Model Validation**: Checks model availability and access permissions
- **Image Processing**: Validates image formats, sizes (max 10MB), and MIME types
- **Upload Failures**: Gracefully handles image upload failures with retry logic
- **Timeout Handling**: Configurable timeouts for all HTTP requests (30-120s)
- **Rate Limiting**: Built-in rate limiting per API key
- **Error Responses**: OpenAI-compatible error format for easy client integration

### Debug Mode

Debug mode is **OFF** by default in production. To enable debugging:

```python
# In src/main.py
DEBUG = True  # Set to True for detailed logging
```

When debug mode is enabled, you'll see:
- Detailed request/response logs
- Image upload progress
- Model capability checks
- Session management details

**Important**: Keep debug mode OFF in production to reduce log verbosity and improve performance.

### Monitoring

Monitor these key metrics in production:

- **API Response Times**: Check for slow responses indicating timeout issues
- **Error Rates**: Track 4xx/5xx errors from `/api/v1/chat/completions`
- **Model Usage**: Dashboard shows top 10 most-used models
- **Image Upload Success**: Monitor image upload failures in logs

### Security Best Practices

1. **API Keys**: Use strong, randomly generated API keys (dashboard auto-generates secure keys)
2. **Rate Limiting**: Configure appropriate rate limits per key in dashboard
3. **Admin Password**: Change default admin password in `config.json`
4. **HTTPS**: Use a reverse proxy (nginx, Caddy) with SSL for production
5. **Firewall**: Restrict access to dashboard port (default 8000)

### Common Issues

**"LMArena API error: An error occurred"**
- Check that your `arena-auth-prod-v1` token is valid
- Verify `cf_clearance` cookie is not expired
- Ensure model is available on LMArena

**Image Upload Failures**
- Verify image is under 10MB
- Check MIME type is supported (image/png, image/jpeg, etc.)
- Ensure LMArena R2 storage is accessible

**Timeout Errors**
- Increase timeout in `src/main.py` if needed (default 120s)
- Check network connectivity to LMArena
- Consider using streaming mode for long responses

### Reverse Proxy Example (Nginx)

```nginx
server {
    listen 443 ssl;
    server_name api.yourdomain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # For streaming responses
        proxy_buffering off;
        proxy_cache off;
    }
}
```

### Running as a Service (systemd)

Create `/etc/systemd/system/lmarenabridge.service`:

```ini
[Unit]
Description=LMArena Bridge API
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/path/to/lmarenabridge
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/python src/main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start:
```bash
sudo systemctl enable lmarenabridge
sudo systemctl start lmarenabridge
sudo systemctl status lmarenabridge
```

## API Reference

### Context Status Feature

#### GET /api/v1/models

Lists available models with their context windows and current usage status.

**Query Parameters:**
- `conversation_id` (optional): Pass conversation ID to see context usage for that conversation

**Response Example:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "gpt-4",
      "object": "model",
      "created": 1704067200,
      "owned_by": "openai",
      "context_window": 128000,
      "context_window_display": "128,000 tokens",
      "context_status": {
        "limit": 128000,
        "used": 1500,
        "remaining": 126500,
        "percentage_used": 1.17,
        "status": "ok",
        "display": "1,500/128,000 tokens used",
        "next_steps": ""
      }
    }
  ]
}
```

**Context Status Fields:**
- `limit`: Maximum tokens for this model
- `used`: Current tokens used in the conversation
- `remaining`: Available tokens remaining
- `percentage_used`: Percentage of context used
- `status`: One of `ok`, `warning` (≥75%), or `critical` (≥90%)
- `display`: Human-readable usage string
- `next_steps`: Actionable advice when approaching limits

#### POST /api/v1/chat/completions

Chat completions endpoint with context tracking.

**Response Fields (in addition to OpenAI format):**
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1704067200,
  "model": "gpt-4",
  "conversation_id": "abc123...",
  "choices": [...],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 200,
    "total_tokens": 350
  },
  "context_status": {
    "limit": 128000,
    "used": 1850,
    "remaining": 126150,
    "percentage_used": 1.45,
    "status": "ok",
    "display": "1,850/128,000 tokens used",
    "next_steps": ""
  }
}
```

For streaming responses, each chunk includes `usage` and `context_status` fields.

#### GET /api/v1/conversations/{conversation_id}/status

Get current context status for a conversation.

**Response Example:**
```json
{
  "conversation_id": "abc123...",
  "model": "gpt-4",
  "messages": 6,
  "context_status": {
    "limit": 128000,
    "used": 2450,
    "remaining": 125550,
    "percentage_used": 1.91,
    "status": "ok",
    "display": "2,450/128,000 tokens used",
    "next_steps": ""
  },
  "usage": {
    "prompt_tokens": 350,
    "completion_tokens": 450,
    "total_tokens": 800
  },
  "updated_at": 1704067200.123
}
```

### Context Window Limits by Model Family

The bridge automatically detects context limits for common model families:

| Model Family | Default Context Window |
|--------------|------------------------|
| GPT-4/GPT-4o | 128,000 tokens |
| GPT-3.5 | 4,096 tokens (16K for 16k variants) |
| Claude 3/3.5 | 200,000 tokens |
| Gemini 2.0/2.5 | 1,000,000 tokens |
| Gemini 1.5 Pro | 2,000,000 tokens |
| Llama 3.1/3.3 | 128,000 tokens |
| Mistral Large | 128,000 tokens |
| DeepSeek v3 | 64,000 tokens |
| Other models | 32,768 tokens (default) |

**Note:** Token estimates use character-based approximation (1 token ≈ 4 characters). Actual token counts may vary slightly depending on the model's tokenizer.
